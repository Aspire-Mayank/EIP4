### Logs:

Train on 60000 samples, validate on 10000 samples
Epoch 1/25

Epoch 00001: LearningRateScheduler setting learning rate to 0.003.
60000/60000 [==============================] - 15s 253us/step - loss: 0.1963 - acc: 0.9417 - val_loss: 0.0710 - val_acc: 0.9760
Epoch 2/25

Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.
60000/60000 [==============================] - 11s 182us/step - loss: 0.0566 - acc: 0.9827 - val_loss: 0.0405 - val_acc: 0.9869
Epoch 3/25

Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0435 - acc: 0.9863 - val_loss: 0.0354 - val_acc: 0.9884
Epoch 4/25

Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0362 - acc: 0.9878 - val_loss: 0.0269 - val_acc: 0.9917
Epoch 5/25

Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0318 - acc: 0.9901 - val_loss: 0.0264 - val_acc: 0.9922
Epoch 6/25

Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0282 - acc: 0.9910 - val_loss: 0.0266 - val_acc: 0.9920
Epoch 7/25

Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.
60000/60000 [==============================] - 11s 182us/step - loss: 0.0251 - acc: 0.9918 - val_loss: 0.0252 - val_acc: 0.9918
Epoch 8/25

Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0245 - acc: 0.9923 - val_loss: 0.0224 - val_acc: 0.9932
Epoch 9/25

Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.
60000/60000 [==============================] - 11s 184us/step - loss: 0.0220 - acc: 0.9928 - val_loss: 0.0219 - val_acc: 0.9934
Epoch 10/25

Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0209 - acc: 0.9934 - val_loss: 0.0249 - val_acc: 0.9928
Epoch 11/25

Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.
60000/60000 [==============================] - 11s 182us/step - loss: 0.0196 - acc: 0.9935 - val_loss: 0.0238 - val_acc: 0.9925
Epoch 12/25

Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.
60000/60000 [==============================] - 11s 182us/step - loss: 0.0181 - acc: 0.9939 - val_loss: 0.0232 - val_acc: 0.9927
Epoch 13/25

Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.
60000/60000 [==============================] - 11s 185us/step - loss: 0.0181 - acc: 0.9942 - val_loss: 0.0207 - val_acc: 0.9940
Epoch 14/25

Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0154 - acc: 0.9951 - val_loss: 0.0203 - val_acc: 0.9939
Epoch 15/25

Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0158 - acc: 0.9949 - val_loss: 0.0256 - val_acc: 0.9923
Epoch 16/25

Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.
60000/60000 [==============================] - 11s 181us/step - loss: 0.0146 - acc: 0.9954 - val_loss: 0.0204 - val_acc: 0.9939
Epoch 17/25

Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.
60000/60000 [==============================] - 11s 181us/step - loss: 0.0130 - acc: 0.9960 - val_loss: 0.0235 - val_acc: 0.9927
Epoch 18/25

Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.
60000/60000 [==============================] - 11s 182us/step - loss: 0.0133 - acc: 0.9957 - val_loss: 0.0198 - val_acc: 0.9939
Epoch 19/25

Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.
60000/60000 [==============================] - 11s 182us/step - loss: 0.0125 - acc: 0.9956 - val_loss: 0.0213 - val_acc: 0.9940
Epoch 20/25

Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
60000/60000 [==============================] - 11s 182us/step - loss: 0.0126 - acc: 0.9961 - val_loss: 0.0213 - val_acc: 0.9935
Epoch 21/25

Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.
60000/60000 [==============================] - 11s 183us/step - loss: 0.0121 - acc: 0.9959 - val_loss: 0.0225 - val_acc: 0.9940
Epoch 22/25

Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.
60000/60000 [==============================] - 11s 182us/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.0213 - val_acc: 0.9938
Epoch 23/25

Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.
60000/60000 [==============================] - 11s 182us/step - loss: 0.0105 - acc: 0.9967 - val_loss: 0.0239 - val_acc: 0.9933
Epoch 24/25

Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.
60000/60000 [==============================] - 11s 181us/step - loss: 0.0101 - acc: 0.9966 - val_loss: 0.0221 - val_acc: 0.9937
Epoch 25/25

Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.
60000/60000 [==============================] - 11s 182us/step - loss: 0.0101 - acc: 0.9968 - val_loss: 0.0206 - val_acc: 0.9945
<keras.callbacks.History at 0x7fa460c96080>

### Result:
[0.020625729440691794, 0.9945]

### Strategy:
> 1. Convloution Layer followed by Batchnorms followed by Dropout.
> 2. before maxpooling used 1x1 to enhanced extacted features extension with BatchNorms.
> 3. Not to use dropout before max pooling.
> 4. I have used two maxpooling and second MaxPooling used two layers before Flatten to get smooth features to carry forward to Softmax.
